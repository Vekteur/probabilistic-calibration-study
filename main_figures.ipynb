{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pickle\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from uq.analysis.constants import (\n",
    "    all_misspec_queries,\n",
    "    model_name,\n",
    "    no_misspec_query,\n",
    ")\n",
    "from uq.analysis.dataframes import (\n",
    "    get_datasets_df,\n",
    "    load_config,\n",
    "    load_df,\n",
    "    make_test_df_for_tuning,\n",
    ")\n",
    "from uq.analysis.plot_boxplots import plot_sorted_boxplot\n",
    "from uq.analysis.plot_calib_all_datasets import (\n",
    "    compute_barplot_order,\n",
    "    plot_calib_all_datasets,\n",
    "    plot_hist_test_statistics,\n",
    "    hyp_test_hists,\n",
    ")\n",
    "from uq.analysis.plot_cd_diagram import draw_my_cd_diagram\n",
    "from uq.analysis.plot_cohen_d import (\n",
    "    build_cohen_d,\n",
    "    plot_cohen_d_boxplot,\n",
    ")\n",
    "from uq.analysis.plot_reliability_diagrams import make_reliability_df, plot_reliability_diagrams\n",
    "from uq.utils.general import filter_dict, savefig, set_notebook_options\n",
    "\n",
    "set_notebook_options()\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "path = Path('results')\n",
    "ext = 'pdf'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard settings in all figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_setting(\n",
    "    df,\n",
    "    interleaved=False,\n",
    "    lambda_=None,\n",
    "    misspec=False,\n",
    "    toy=False,\n",
    "    posthoc=True,\n",
    "    add_posthoc_dataset=True,\n",
    "):\n",
    "    df = df.query('s.isna() or s == 100')\n",
    "    df = df.query('n_quantiles.isna() or n_quantiles == 64')\n",
    "\n",
    "    if toy:\n",
    "        df = df.query('dataset_group == \"toy\"')\n",
    "    else:\n",
    "        df = df.query('dataset_group != \"toy\"')\n",
    "    if not interleaved:\n",
    "        df = df.query('not interleaved or interleaved.isna()')\n",
    "    if lambda_ is not None:\n",
    "        df = df.query('lambda_ in @lambda_ or lambda_.isna()')\n",
    "    if 'misspecification' in df.index.names and not misspec:\n",
    "        df = df.query(no_misspec_query)\n",
    "    if not posthoc:\n",
    "        df = df.query('posthoc_method.isna()')\n",
    "    index = df.index.names\n",
    "    df = df.reset_index()\n",
    "    model_name_partial = partial(model_name, add_posthoc_dataset=add_posthoc_dataset)\n",
    "    df['name'] = df.apply(model_name_partial, axis='columns').astype('string')\n",
    "    df['base_loss'] = pd.Categorical(df['base_loss'], ['nll', 'crps', 'expected_qs']).astype('string')\n",
    "    df.sort_values('base_loss', kind='stable')\n",
    "    df = df.set_index(index + ['name'])\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('logs/full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df(config, tuning=True)\n",
    "test_df = make_test_df_for_tuning(df, config)\n",
    "test_df['drop_prob'] = np.nan\n",
    "test_df = test_df.set_index('drop_prob', append=True)\n",
    "\n",
    "\n",
    "def op_without_index(df, op):\n",
    "    names = df.index.names\n",
    "    df = op(df.reset_index())\n",
    "    return df.set_index(names)\n",
    "\n",
    "\n",
    "def op(df):\n",
    "    if 'lambda_' in df.columns:\n",
    "        df['lambda_'] = df['lambda_'].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "test_df = op_without_index(test_df, op)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of best $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_query = 'model == \"no_regul\"'\n",
    "join_by = [\n",
    "    'dataset_group',\n",
    "    'dataset',\n",
    "    'run_id',\n",
    "    'nb_hidden',\n",
    "    'drop_prob',\n",
    "    'base_loss',\n",
    "    'pred_type',\n",
    "    'mixture_size',\n",
    "    'posthoc_method',\n",
    "    'posthoc_dataset',\n",
    "]\n",
    "\n",
    "\n",
    "def duplicate_baseline_per_regul(df):\n",
    "    df_baseline = df.query(baseline_query)\n",
    "    df_regul = df.query(f'not ({baseline_query})')\n",
    "    # Get all regularization groups\n",
    "    groups = df_regul.groupby(join_by + ['model'], dropna=False).size()\n",
    "    # Convert the result to a dataframe\n",
    "    groups = groups.index.to_frame().reset_index(drop=True)\n",
    "    # Get the baseline for each regularization group\n",
    "    df_baseline_per_regul = groups.merge(df_baseline, how='left', on=join_by)\n",
    "    df_baseline_per_regul['lambda_'] = 0\n",
    "    index = df_regul.index.names\n",
    "    concat = pd.concat([df_regul.reset_index(), df_baseline_per_regul])\n",
    "    return concat.set_index(index)\n",
    "\n",
    "\n",
    "tuned_lambda_values = [0, 0.01, 0.05, 0.2, 1, 5]\n",
    "test_df_dup = duplicate_baseline_per_regul(test_df.query('lambda_.isna() or lambda_ in @tuned_lambda_values'))\n",
    "test_df_dup.reset_index().lambda_.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['lambda_']\n",
    "accepted_relative_wis_loss = 0.1\n",
    "\n",
    "\n",
    "def make_test_df_mean(test_df):\n",
    "    test_df = test_df.reset_index(level='run_id', drop=True)\n",
    "    return test_df.groupby(test_df.index.names, dropna=False).mean()\n",
    "\n",
    "\n",
    "def select_best_lambda(test_df_dup):\n",
    "    # For more stable results, we compare the *mean* WIS and calib_l1 per model\n",
    "    df = test_df_dup[['val_wis', 'val_calib_l1']]\n",
    "    df = make_test_df_mean(test_df_dup)\n",
    "    # We compare each model (include models without regularization) to their baseline (i.e., same model without regularization)\n",
    "    baseline = df.query('lambda_ == 0')[['val_wis']]\n",
    "    df = df.reset_index(level=columns_to_keep)\n",
    "    join_by_for_mean = join_by + ['model']\n",
    "    join_by_for_mean.remove('run_id')\n",
    "    df = df.merge(baseline, how='left', on=join_by_for_mean, suffixes=(None, '_baseline'), validate='many_to_one')\n",
    "    # Constraint on WIS (models without regularization are guaranteed to be selected)\n",
    "    mask = df['val_wis'] <= df['val_wis_baseline'] * (1 + accepted_relative_wis_loss)\n",
    "    df = df[mask]\n",
    "    df = df.set_index('lambda_', append=True)\n",
    "    # Take lambda with minimum calibration per model\n",
    "    idxmin = df.groupby(join_by_for_mean + ['model'], dropna=False)['val_calib_l1'].idxmin()\n",
    "    df = df.loc[idxmin]\n",
    "    selected_index = df.index.to_frame().reset_index(drop=True)\n",
    "    index_names = test_df_dup.index.names\n",
    "    df = selected_index.merge(test_df_dup.reset_index(), on=join_by_for_mean + ['lambda_'])\n",
    "    df = df.set_index(index_names)\n",
    "    return df\n",
    "\n",
    "\n",
    "test_df_best = select_best_lambda(test_df_dup)\n",
    "test_df = pd.concat([test_df_best, test_df.query(baseline_query)])\n",
    "test_df_best.reset_index().lambda_.value_counts(), len(test_df_best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's d figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['test_calib_l1', 'test_wis', 'test_nll', 'test_stddev']\n",
    "more_metrics = ['test_calib_l2', 'test_rmse', 'test_mae']\n",
    "\n",
    "\n",
    "def save_cohen_d(*args, path=None, **kwargs):\n",
    "    path = Path(path)\n",
    "    fig = plot_cohen_d_boxplot(*args, **kwargs)\n",
    "    savefig(path / f'cohen_d_boxplot.{ext}', fig)\n",
    "\n",
    "\n",
    "def metric_queries(metrics):\n",
    "    return {metric: f'metric == \"{metric}\"' for metric in metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_cohen_d(test_df, default_cohen_d, path, cd_diagrams=True, fig_kwargs={}, **kwargs):\n",
    "    # All base losses\n",
    "    add_posthoc_dataset = test_df.reset_index()['posthoc_dataset'].nunique() > 1\n",
    "    plot_df = standard_setting(test_df, add_posthoc_dataset=add_posthoc_dataset, **kwargs)\n",
    "    df_cohen = default_cohen_d(plot_df)\n",
    "    save_cohen_d(df_cohen, metric_queries(metrics), legend=False, path=path / 'main_metrics', **fig_kwargs)\n",
    "    if cd_diagrams:\n",
    "        for metric in metrics:\n",
    "            try:\n",
    "                draw_my_cd_diagram(plot_df, metric)\n",
    "            except ValueError:\n",
    "                print(f'The friedman test failed for {metric}')\n",
    "            savefig(path / 'cd_diagrams' / f'{metric}.{ext}', dpi=300)\n",
    "\n",
    "    # All base losses with other metrics\n",
    "    df_cohen = default_cohen_d(plot_df, metrics=more_metrics)\n",
    "    save_cohen_d(df_cohen, metric_queries(more_metrics), legend=False, path=path / 'more_metrics', **fig_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of some post-hoc and regularization vs baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline is precisely no regularization, no post-hoc, mixture prediction and nll loss\n",
    "baseline_query = 'model == \"no_regul\" and posthoc_method.isna() and pred_type == \"mixture\" and base_loss == \"nll\"'\n",
    "join_by = [\n",
    "    'dataset_group',\n",
    "    'dataset',\n",
    "    'run_id',\n",
    "    'metric',\n",
    "    'misspecification',\n",
    "    'nb_hidden',\n",
    "    'drop_prob',\n",
    "]\n",
    "columns_to_keep = [\n",
    "    'base_loss',\n",
    "    'pred_type',\n",
    "    'mixture_size',\n",
    "    'model',\n",
    "    'posthoc_method',\n",
    "    'posthoc_dataset',\n",
    "    'name',\n",
    "]\n",
    "\n",
    "cases = [\n",
    "    baseline_query,\n",
    "    'model == \"no_regul\" and (posthoc_method in [\"CQR\", \"rec-kde\", \"rec-lin\", \"rec-emp\"] or posthoc_method.isna())',\n",
    "    'model in [\"cdf_based\", \"entropy_based\", \"truncated\"] and posthoc_method.isna()',\n",
    "]\n",
    "query = ' or '.join(f'({case})' for case in cases)\n",
    "new_df = test_df.query(query)\n",
    "new_df = new_df.query('posthoc_dataset.isna() or posthoc_dataset == \"calib\"')\n",
    "\n",
    "for metric in metrics:\n",
    "    default_cohen_d = partial(\n",
    "        build_cohen_d,\n",
    "        metrics=[metric],\n",
    "        baseline_query=baseline_query,\n",
    "        join_by=join_by,\n",
    "        columns_to_keep=columns_to_keep,\n",
    "    )\n",
    "\n",
    "    plot_all_cohen_d(\n",
    "        new_df,\n",
    "        default_cohen_d,\n",
    "        path=path / 'posthoc_and_regul_vs_baseline' / metric,\n",
    "        fig_kwargs={'color_map_name': 'posthoc_or_regul'},\n",
    "        cd_diagrams=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline is precisely no regularization, no post-hoc, mixture prediction and nll loss\n",
    "baseline_query = 'model == \"no_regul\" and posthoc_method.isna() and pred_type == \"mixture\" and base_loss == \"nll\"'\n",
    "join_by = [\n",
    "    'dataset_group',\n",
    "    'dataset',\n",
    "    'run_id',\n",
    "    'metric',\n",
    "    'misspecification',\n",
    "    'nb_hidden',\n",
    "    'drop_prob',\n",
    "]\n",
    "columns_to_keep = [\n",
    "    'base_loss',\n",
    "    'pred_type',\n",
    "    'mixture_size',\n",
    "    'model',\n",
    "    'posthoc_method',\n",
    "    'posthoc_dataset',\n",
    "    'name',\n",
    "]\n",
    "\n",
    "default_cohen_d = partial(\n",
    "    build_cohen_d,\n",
    "    metrics=metrics,\n",
    "    baseline_query=baseline_query,\n",
    "    join_by=join_by,\n",
    "    columns_to_keep=columns_to_keep,\n",
    ")\n",
    "\n",
    "cases = [\n",
    "    baseline_query,\n",
    "    'model == \"no_regul\" and (posthoc_method in [\"CQR\", \"rec-kde\", \"rec-lin\", \"rec-emp\"] or posthoc_method.isna())',\n",
    "    'model in [\"cdf_based\", \"entropy_based\", \"truncated\"] and posthoc_method.isna()',\n",
    "]\n",
    "query = ' or '.join(f'({case})' for case in cases)\n",
    "new_df = test_df.query(query)\n",
    "new_df = new_df.query('posthoc_dataset.isna() or posthoc_dataset == \"calib\"')\n",
    "\n",
    "plot_all_cohen_d(\n",
    "    new_df,\n",
    "    default_cohen_d,\n",
    "    path=path / 'posthoc_and_regul_vs_baseline',\n",
    "    fig_kwargs={'color_map_name': 'posthoc_or_regul'},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between post-hoc on training and calibration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline is precisely no regularization, no post-hoc, mixture prediction and nll loss\n",
    "baseline_query = 'model == \"no_regul\" and posthoc_method.isna() and pred_type == \"mixture\" and base_loss == \"nll\"'\n",
    "join_by = [\n",
    "    'dataset_group',\n",
    "    'dataset',\n",
    "    'run_id',\n",
    "    'metric',\n",
    "    'misspecification',\n",
    "    'nb_hidden',\n",
    "    'drop_prob',\n",
    "]\n",
    "columns_to_keep = [\n",
    "    'base_loss',\n",
    "    'pred_type',\n",
    "    'mixture_size',\n",
    "    'model',\n",
    "    'posthoc_method',\n",
    "    'posthoc_dataset',\n",
    "    'name',\n",
    "]\n",
    "\n",
    "default_cohen_d = partial(\n",
    "    build_cohen_d,\n",
    "    metrics=metrics,\n",
    "    baseline_query=baseline_query,\n",
    "    join_by=join_by,\n",
    "    columns_to_keep=columns_to_keep,\n",
    ")\n",
    "\n",
    "cases = [\n",
    "    baseline_query,\n",
    "    'model == \"no_regul\" and posthoc_method in [\"CQR\", \"rec-kde\", \"rec-lin\"]',\n",
    "    'model == \"cdf_based\" and posthoc_method.isna()',\n",
    "]\n",
    "query = ' or '.join(f'({case})' for case in cases)\n",
    "new_df = test_df.query(query)\n",
    "new_df = new_df.query('base_loss in [\"nll\", \"expected_qs\"]')\n",
    "\n",
    "# new_df = test_df.query('posthoc_method.isna() or posthoc_method in [\"CQR\", \"rec-kde\", \"rec-lin\"]')\n",
    "# new_df = new_df.query('model == \"no_regul\" or (model == \"cdf_based\" and posthoc_method.isna())')\n",
    "# new_df = new_df.query('base_loss in [\"nll\", \"expected_qs\"]')\n",
    "plot_all_cohen_d(\n",
    "    new_df,\n",
    "    default_cohen_d,\n",
    "    path=path / 'posthoc_dataset_train_vs_calib',\n",
    "    fig_kwargs={'color_map_name': 'posthoc_dataset'},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of regularization or posthoc vs vanilla per base loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline is precisely no regularization, no post-hoc, mixture prediction and nll loss\n",
    "baseline_query = 'model == \"no_regul\" and posthoc_method.isna()'\n",
    "join_by = [\n",
    "    'dataset_group',\n",
    "    'dataset',\n",
    "    'run_id',\n",
    "    'metric',\n",
    "    'misspecification',\n",
    "    'nb_hidden',\n",
    "    'drop_prob',\n",
    "    'base_loss',\n",
    "    'pred_type',\n",
    "    'mixture_size',\n",
    "]\n",
    "columns_to_keep = ['model', 'posthoc_method', 'posthoc_dataset', 'name']\n",
    "\n",
    "default_cohen_d = partial(\n",
    "    build_cohen_d,\n",
    "    metrics=metrics,\n",
    "    baseline_query=baseline_query,\n",
    "    join_by=join_by,\n",
    "    columns_to_keep=columns_to_keep,\n",
    ")\n",
    "\n",
    "for base_loss, base_loss_df in test_df.groupby('base_loss'):\n",
    "    print(base_loss, flush=True)\n",
    "    new_df = base_loss_df.query('model == \"no_regul\" or posthoc_method.isna()')\n",
    "    new_df = new_df.query('posthoc_dataset == \"calib\" or posthoc_dataset.isna()')\n",
    "    plot_all_cohen_d(\n",
    "        new_df,\n",
    "        default_cohen_d,\n",
    "        path=path / 'posthoc_or_regul_vs_vanilla' / base_loss,\n",
    "        fig_kwargs={'color_map_name': 'posthoc_or_regul'},\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of ALL post-hoc and regularization vs baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline is precisely no regularization, no post-hoc, mixture prediction and nll loss\n",
    "baseline_query = 'model == \"no_regul\" and posthoc_method.isna() and pred_type == \"mixture\" and base_loss == \"nll\"'\n",
    "join_by = [\n",
    "    'dataset_group',\n",
    "    'dataset',\n",
    "    'run_id',\n",
    "    'metric',\n",
    "    'misspecification',\n",
    "    'nb_hidden',\n",
    "    'drop_prob',\n",
    "]\n",
    "columns_to_keep = [\n",
    "    'base_loss',\n",
    "    'pred_type',\n",
    "    'mixture_size',\n",
    "    'model',\n",
    "    'posthoc_method',\n",
    "    'posthoc_dataset',\n",
    "    'name',\n",
    "]\n",
    "\n",
    "default_cohen_d = partial(\n",
    "    build_cohen_d,\n",
    "    metrics=metrics,\n",
    "    baseline_query=baseline_query,\n",
    "    join_by=join_by,\n",
    "    columns_to_keep=columns_to_keep,\n",
    ")\n",
    "\n",
    "plot_all_cohen_d(\n",
    "    test_df,\n",
    "    default_cohen_d,\n",
    "    path=path / 'all_posthoc_and_regul_vs_baseline',\n",
    "    fig_kwargs={'figsize': (20, 10)},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of regularization methods vs no regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `baseline_query` selects the baselines. The models that are not selected are the compared models.\n",
    "baseline_query = 'model == \"no_regul\"'\n",
    "# `join_by` represents all columns that should be the same when comparing a model and its baseline.\n",
    "join_by = [\n",
    "    'dataset_group',\n",
    "    'dataset',\n",
    "    'run_id',\n",
    "    'metric',\n",
    "    'misspecification',\n",
    "    'nb_hidden',\n",
    "    'drop_prob',\n",
    "    'base_loss',\n",
    "    'pred_type',\n",
    "    'mixture_size',\n",
    "]\n",
    "# `columns_to_keep` represents the columns of the compared model that should be kept in the final result.\n",
    "# Note that these columns do not have to be the same between a compared model and its baseline.\n",
    "columns_to_keep = ['model', 'name']\n",
    "\n",
    "default_cohen_d = partial(\n",
    "    build_cohen_d,\n",
    "    metrics=metrics,\n",
    "    baseline_query=baseline_query,\n",
    "    join_by=join_by,\n",
    "    columns_to_keep=columns_to_keep,\n",
    ")\n",
    "\n",
    "plot_all_cohen_d(test_df.query('posthoc_method.isna()'), default_cohen_d, path=path / 'regul_vs_no_regul')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of post-hoc methods vs no post-hoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_query = 'posthoc_method.isna()'\n",
    "join_by = [\n",
    "    'dataset_group',\n",
    "    'dataset',\n",
    "    'run_id',\n",
    "    'metric',\n",
    "    'misspecification',\n",
    "    'nb_hidden',\n",
    "    'drop_prob',\n",
    "    'base_loss',\n",
    "    'pred_type',\n",
    "    'mixture_size',\n",
    "    'model',\n",
    "]\n",
    "columns_to_keep = ['posthoc_method', 'posthoc_dataset', 'name']\n",
    "\n",
    "default_cohen_d = partial(\n",
    "    build_cohen_d,\n",
    "    metrics=metrics,\n",
    "    baseline_query=baseline_query,\n",
    "    join_by=join_by,\n",
    "    columns_to_keep=columns_to_keep,\n",
    ")\n",
    "\n",
    "plot_all_cohen_d(test_df.query('model == \"no_regul\"'), default_cohen_d, path=path / 'posthoc_vs_no_posthoc')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all methods (vanilla, post-hoc and regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_methods(test_df, path):\n",
    "    df = standard_setting(test_df, posthoc=True, add_posthoc_dataset=True)\n",
    "    df = df.query('posthoc_method.isna() or posthoc_method in [\"CQR\", \"rec-kde\"]')\n",
    "    df = df.query('model == \"no_regul\" or (model == \"cdf_based\" and posthoc_method.isna())')\n",
    "\n",
    "    metrics = [\n",
    "        'PCE',\n",
    "        'CRPS',\n",
    "        'NLL',\n",
    "    ]\n",
    "    df = df.rename(columns={'test_calib_l1': 'PCE', 'test_wis': 'CRPS', 'test_nll': 'NLL'})\n",
    "    plot_sorted_boxplot(df, metrics=metrics)\n",
    "    savefig(path / f'boxplot.{ext}')\n",
    "    for metric in metrics:\n",
    "        draw_my_cd_diagram(df, metric)\n",
    "        savefig(path / 'cd_diagrams' / f'{metric}.{ext}', dpi=300)\n",
    "\n",
    "\n",
    "plot_all_methods(test_df, path=path / 'all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = test_df.query('posthoc_method.isna() and model == \"no_regul\"')\n",
    "plot_df = standard_setting(plot_df)\n",
    "rel_df = make_reliability_df(plot_df)\n",
    "fig = plot_reliability_diagrams(rel_df, ncols=7, agg_run=True)\n",
    "savefig(path / 'rel_diags' / f'vanilla.{ext}')\n",
    "\n",
    "plot_df = standard_setting(test_df)\n",
    "rel_df = make_reliability_df(plot_df)\n",
    "fig = plot_reliability_diagrams(rel_df, ncols=7, agg_run=True)\n",
    "savefig(path / 'rel_diags' / f'regul.{ext}')\n",
    "\n",
    "plot_df = test_df.query('posthoc_method in [\"rec-emp\", \"CQR\"] and posthoc_dataset == \"calib\" and model == \"no_regul\"')\n",
    "plot_df = standard_setting(plot_df, add_posthoc_dataset=False)\n",
    "rel_df = make_reliability_df(plot_df)\n",
    "fig = plot_reliability_diagrams(rel_df, ncols=7, agg_run=True)\n",
    "savefig(path / 'rel_diags' / f'posthoc.{ext}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = standard_setting(test_df, add_posthoc_dataset=False).query(\n",
    "    'posthoc_dataset.isna() or posthoc_dataset == \"calib\"'\n",
    ")\n",
    "order = compute_barplot_order(plot_df, 'name == \"MIX-NLL\"', 'test_calib_l1')\n",
    "order.to_pickle(Path(config.log_dir) / 'order.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_statistics_path = path / 'test_statistics.pkl'\n",
    "if test_statistics_path.exists():\n",
    "    with open(test_statistics_path, 'rb') as f:\n",
    "        test_statistics = pickle.load(f)\n",
    "else:\n",
    "    test_statistics = hyp_test_hists(plot_df, config, nb_test_samples=10000)\n",
    "    with open(test_statistics_path, 'wb') as f:\n",
    "        pickle.dump(test_statistics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by_nb_instances = False\n",
    "if sort_by_nb_instances:\n",
    "    order = get_datasets_df(config, reload=True).reset_index().sort_values('Total instances').Dataset\n",
    "    datasets = plot_df.reset_index().dataset.unique()\n",
    "    order = order[order.isin(datasets)]\n",
    "plot_calib_all_datasets(\n",
    "    plot_df,\n",
    "    config,\n",
    "    order,\n",
    "    names=['MIX-NLL'],\n",
    "    test_statistics=test_statistics,\n",
    "    path=path / 'pce_and_rel_diags' / f'vanilla_nll.{ext}',\n",
    ")\n",
    "plot_calib_all_datasets(\n",
    "    plot_df,\n",
    "    config,\n",
    "    order,\n",
    "    names=['MIX-CRPS'],\n",
    "    test_statistics=test_statistics,\n",
    "    path=path / 'pce_and_rel_diags' / f'vanilla_crps.{ext}',\n",
    ")\n",
    "plot_calib_all_datasets(\n",
    "    plot_df,\n",
    "    config,\n",
    "    order,\n",
    "    names=['SQR-CRPS'],\n",
    "    test_statistics=test_statistics,\n",
    "    path=path / 'pce_and_rel_diags' / f'vanilla_wis.{ext}',\n",
    ")\n",
    "plot_calib_all_datasets(\n",
    "    plot_df,\n",
    "    config,\n",
    "    order,\n",
    "    names=['MIX-NLL + Rec-EMP'],\n",
    "    test_statistics=test_statistics,\n",
    "    path=path / 'pce_and_rel_diags' / f'posthoc_nll.{ext}',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of test statistics distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mpl.rc_context({'axes.formatter.limits': (-3, 4)}):\n",
    "    plot_hist_test_statistics(\n",
    "        plot_df, config, order, test_statistics, path=path / 'pce_and_rel_diags' / f'hist_test_statistic.{ext}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparison of posthoc + regularization vs posthoc\n",
    "# The baseline is precisely no regularization, no post-hoc, mixture prediction and nll loss\n",
    "baseline_query = 'model == \"no_regul\"'\n",
    "join_by = [\n",
    "    'dataset_group',\n",
    "    'dataset',\n",
    "    'run_id',\n",
    "    'metric',\n",
    "    'nb_hidden',\n",
    "    'drop_prob',\n",
    "    'base_loss',\n",
    "    'pred_type',\n",
    "    'mixture_size',\n",
    "    'posthoc_method',\n",
    "    'posthoc_dataset',\n",
    "]\n",
    "columns_to_keep = [\n",
    "    'model',\n",
    "    'name',\n",
    "]\n",
    "\n",
    "default_cohen_d = partial(\n",
    "    build_cohen_d,\n",
    "    metrics=metrics,\n",
    "    baseline_query=baseline_query,\n",
    "    join_by=join_by,\n",
    "    columns_to_keep=columns_to_keep,\n",
    ")\n",
    "\n",
    "query_pairs = [\n",
    "    'posthoc_method == \"rec-kde\" and model in [\"no_regul\", \"cdf_based\"]',\n",
    "    'posthoc_method == \"rec-kde\" and model in [\"no_regul\", \"entropy_based\"]',\n",
    "    'posthoc_method == \"rec-lin\" and model in [\"no_regul\", \"cdf_based\"]',\n",
    "    'posthoc_method == \"CQR\" and model in [\"no_regul\", \"truncated\"]',\n",
    "]\n",
    "query = ' or '.join(f'({pair})' for pair in query_pairs)\n",
    "\n",
    "plot_all_cohen_d(\n",
    "    test_df.query(query).query('posthoc_dataset == \"calib\"'),\n",
    "    default_cohen_d,\n",
    "    path=path / 'posthoc_and_regul_vs_posthoc',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of posthoc + regularization vs posthoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('logs/full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df(config, tuning=True)\n",
    "test_df = make_test_df_for_tuning(df, config)\n",
    "test_df['drop_prob'] = np.nan\n",
    "test_df = test_df.set_index('drop_prob', append=True)\n",
    "test_df = op_without_index(test_df, op)\n",
    "\n",
    "baseline_query = 'model == \"no_regul\"'\n",
    "join_by = [\n",
    "    'dataset_group',\n",
    "    'dataset',\n",
    "    'run_id',\n",
    "    'nb_hidden',\n",
    "    'drop_prob',\n",
    "    'base_loss',\n",
    "    'pred_type',\n",
    "    'mixture_size',\n",
    "    'posthoc_method',\n",
    "    'posthoc_dataset',\n",
    "]\n",
    "tuned_lambda_values = [0, 0.01, 0.05, 0.2, 1, 5]\n",
    "test_df_dup = duplicate_baseline_per_regul(test_df.query('lambda_.isna() or lambda_ in @tuned_lambda_values'))\n",
    "\n",
    "columns_to_keep = ['lambda_']\n",
    "accepted_relative_wis_loss = 0.1\n",
    "test_df_best = select_best_lambda(test_df_dup)\n",
    "test_df = pd.concat([test_df_best, test_df.query(baseline_query)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline is precisely no regularization, no post-hoc, mixture prediction and nll loss\n",
    "baseline_query = 'model == \"no_regul\"'\n",
    "join_by = [\n",
    "    'dataset_group',\n",
    "    'dataset',\n",
    "    'run_id',\n",
    "    'metric',\n",
    "    'nb_hidden',\n",
    "    'drop_prob',\n",
    "    'base_loss',\n",
    "    'pred_type',\n",
    "    'mixture_size',\n",
    "    'posthoc_method',\n",
    "    'posthoc_dataset',\n",
    "]\n",
    "columns_to_keep = [\n",
    "    'model',\n",
    "    'name',\n",
    "]\n",
    "\n",
    "default_cohen_d = partial(\n",
    "    build_cohen_d,\n",
    "    metrics=metrics,\n",
    "    baseline_query=baseline_query,\n",
    "    join_by=join_by,\n",
    "    columns_to_keep=columns_to_keep,\n",
    ")\n",
    "\n",
    "query_pairs = [\n",
    "    'posthoc_method == \"rec-kde\" and model in [\"no_regul\", \"cdf_based\"]',\n",
    "    'posthoc_method == \"rec-kde\" and model in [\"no_regul\", \"entropy_based\"]',\n",
    "    'posthoc_method == \"rec-lin\" and model in [\"no_regul\", \"cdf_based\"]',\n",
    "    'posthoc_method == \"CQR\" and model in [\"no_regul\", \"truncated\"]',\n",
    "]\n",
    "query = ' or '.join(f'({pair})' for pair in query_pairs)\n",
    "\n",
    "plot_all_cohen_d(\n",
    "    test_df.query(query).query('posthoc_dataset == \"calib\"'),\n",
    "    default_cohen_d,\n",
    "    path=path / 'posthoc_and_regul_vs_posthoc',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a8dedf9293b37ff9b138d946f5378258f0cf3b04aeac4c8b4c7c642523b0a45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
